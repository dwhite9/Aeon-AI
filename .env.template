# Aeon AI Platform - Environment Configuration Template
# Copy this file to .env and customize for your environment
# IMPORTANT: Never commit .env to version control!
#
# This file serves as both documentation and default configuration.
# All values shown here are the defaults used if not specified in your .env file.

# ============================================================================
# SYSTEM PATHS
# ============================================================================

# Temporary directory for container builds (should have plenty of space)
# Default: /tmp (system temp directory)
# Recommendation: Use a partition with at least 20GB free for building GPU images
TMPDIR=/tmp

# Podman runtime directory for temporary files
# Default: /tmp/podman/runroot
# Only needed if using Podman
#PODMAN_RUNROOT=/tmp/podman/runroot

# ============================================================================
# NETWORK CONFIGURATION
# ============================================================================

# Host IP address where inference services (vLLM, embeddings) will run
# This is used by K8s pods to reach services running on the host
# Default: 192.168.1.100 (CHANGE THIS to your actual host IP)
# To auto-detect: ip route get 1.1.1.1 | grep -oP 'src \K[^ ]+'
HOST_IP=192.168.1.100

# ============================================================================
# DATABASE CREDENTIALS
# ============================================================================

# PostgreSQL credentials for analytics and query logging
# Default database name
POSTGRES_DB=aiplatform
# Default user
POSTGRES_USER=aiuser
# DEFAULT PASSWORD - CHANGE IN PRODUCTION!
POSTGRES_PASSWORD=changeme_to_secure_password
# PostgreSQL service name (K8s service name or container name)
# Default: postgres-postgresql (Helm chart default)
POSTGRES_HOST=postgres-postgresql
# PostgreSQL port
# Default: 5432
POSTGRES_PORT=5432

# Redis configuration for caching
# Redis service name (K8s service name or container name)
# Default: redis-master (Helm chart default)
REDIS_HOST=redis-master
# Redis port
# Default: 6379
REDIS_PORT=6379
# Optional: Redis password (if authentication enabled)
#REDIS_PASSWORD=

# ============================================================================
# SERVICE ENDPOINTS
# ============================================================================

# vLLM inference endpoint (running on host)
# Default: http://HOST_IP:8000/v1
# Note: HOST_IP will be substituted with the value above
VLLM_ENDPOINT=http://192.168.1.100:8000/v1

# Embedding service endpoint (running on host)
# Default: http://HOST_IP:8001
EMBEDDING_ENDPOINT=http://192.168.1.100:8001

# Qdrant vector database (K8s service)
# Default: http://qdrant.vector-db:6333
QDRANT_HOST=http://qdrant.vector-db:6333

# SearXNG search engine (K8s service, if deployed)
# Default: http://searxng.search-engine:8080
SEARXNG_ENDPOINT=http://searxng.search-engine:8080

# ============================================================================
# FEATURE FLAGS
# ============================================================================

# Enable RAG (Retrieval-Augmented Generation) functionality
# Default: true
ENABLE_RAG=true

# Enable AI agent with tool use (requires vLLM)
# Default: true
ENABLE_AGENT=true

# Enable code execution sandbox (requires K8s)
# Default: true
ENABLE_CODE_EXEC=true

# Enable query analytics and logging
# Default: true
ENABLE_ANALYTICS=true

# ============================================================================
# CONTAINER REGISTRY
# ============================================================================

# Local container registry for K8s deployments
# Default: localhost:5000
# This is used for pushing/pulling container images to K3s
REGISTRY_URL=localhost:5000

# ============================================================================
# LLM MODEL CONFIGURATION
# ============================================================================

# HuggingFace Hub Token (required for private/gated models)
# Get your token from: https://huggingface.co/settings/tokens
# Required for models like:
#   - casperhansen/qwen2.5-14b-instruct-awq
#   - Other private or gated repositories
# Leave empty for public models
# IMPORTANT: Keep this secret! Never commit .env to version control!
#HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Model name for vLLM to load
# Options:
#   - mistralai/Mistral-7B-Instruct-v0.3 (7B, good for basic tasks)
#   - Qwen/Qwen2.5-14B-Instruct (14B, excellent tool use, 9-10GB VRAM)
#   - TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ (47B MoE, best quality, 10-11GB VRAM)
#   - casperhansen/qwen2.5-14b-instruct-awq (14B AWQ quantized, requires HF_TOKEN)
# Default: Qwen/Qwen2.5-14B-Instruct
MODEL_NAME=Qwen/Qwen2.5-14B-Instruct

# Quantization method
# Options: bitsandbytes (8-bit), awq (4-bit), gptq (4-bit), squeezellm
# Default: bitsandbytes (8-bit, best quality vs speed)
# Note: Use 'awq' for 4-bit quantized models (e.g., TheBloke models)
QUANTIZATION=bitsandbytes

# GPU memory utilization for vLLM (0.0 to 1.0)
# Default: 0.6 (60% of GPU memory)
# Recommendation: 0.6-0.7 for single GPU, leaves room for embeddings
GPU_MEMORY_UTILIZATION=0.6

# Maximum model context length (tokens)
# Default: 8192
# Note: Longer contexts use more memory
MAX_MODEL_LENGTH=8192

# ============================================================================
# EMBEDDING MODEL CONFIGURATION
# ============================================================================

# Embedding model name
# Default: sentence-transformers/all-MiniLM-L6-v2
#EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Embedding dimension
# Default: 384 (for all-MiniLM-L6-v2)
#EMBEDDING_DIM=384

# ============================================================================
# RAG CONFIGURATION
# ============================================================================

# Document chunk size (characters)
# Default: 3200 (~600-800 tokens)
#CHUNK_SIZE=3200

# Chunk overlap (characters)
# Default: 600 (~150 tokens)
#CHUNK_OVERLAP=600

# Number of top documents to retrieve
# Default: 3
#RAG_TOP_K=3

# Similarity threshold for retrieval (0.0 to 1.0)
# Default: 0.7
#RAG_SIMILARITY_THRESHOLD=0.7

# ============================================================================
# CACHE CONFIGURATION
# ============================================================================

# Exact cache TTL (seconds)
# Default: 3600 (1 hour)
#CACHE_TTL=3600

# Semantic cache similarity threshold (0.0 to 1.0)
# Default: 0.95
#SEMANTIC_CACHE_THRESHOLD=0.95

# ============================================================================
# MONITORING
# ============================================================================

# Grafana dashboard access
# Grafana NodePort (K8s)
# Default: 30001
GRAFANA_PORT=30001
# Grafana admin username
# Default: admin
GRAFANA_USER=admin
# Grafana admin password
# DEFAULT PASSWORD - CHANGE IN PRODUCTION!
GRAFANA_PASSWORD=prom-operator

# Prometheus retention period
# Default: 15d
#PROMETHEUS_RETENTION=15d

# ============================================================================
# CODE EXECUTION SANDBOX
# ============================================================================

# Maximum execution time for code (seconds)
# Default: 30
#CODE_EXEC_TIMEOUT=30

# Maximum memory for code execution (MB)
# Default: 1024 (1GB)
#CODE_EXEC_MEMORY_LIMIT=1024

# Maximum CPU cores for code execution
# Default: 1
#CODE_EXEC_CPU_LIMIT=1

# ============================================================================
# DEPLOYMENT MODE
# ============================================================================

# Deployment environment
# Options: development, production
# Default: production
#ENVIRONMENT=production

# Log level
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Default: INFO
#LOG_LEVEL=INFO

# Enable debug mode (verbose logging, hot reload)
# Default: false
#DEBUG=false
